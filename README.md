# ML Challenge 2025: Smart Product Pricing Solution Template
 

---

## ğŸ“„ Table of Contents

- [Project Overview](#project-overview)  
- [Problem Statement](#problem-statement)  
- [Solution Approach](#solution-approach)  
- [Results & Performance](#results--performance)  
- [Folder Structure](#folder-structure)  
- [Getting Started / Setup](#getting-started--setup)  
- [Requirements](#requirements)  
- [How to Run](#how-to-run)  
- [Modeling Details](#modeling-details)  
- [Evaluation Metric (SMAPE)](#evaluation-metric-smap e)  
- [Challenges & Lessons Learned](#challenges--lessons-learned)  
- [Future Work & Extensions](#future-work--extensions)  
- [Acknowledgements & References](#acknowledgements--references)  

---

## ğŸ” Project Overview

This repository contains our solution for â€œSmart Product Pricingâ€ â€” a multimodal machine-learning approach (text + image features) to estimate product prices given product metadata and images.  
We engineered a combined feature set (textual, TF-IDF, image) and trained an ensemble of gradient-boosting models to deliver robust price predictions.

---

## ğŸ§© Problem Statement

We have a dataset of products with attributes like `sample_id`, `catalog_content`, `image_link`, and (for training) `price`.  
Goal: for test samples, predict product price as accurately as possible.  
Key challenges:  
- Price distribution is heavily right-skewed with outliers.  
- Product descriptions vary significantly in structure and length.  
- Not all products have images; for some, image quality or availability may be poor.  
- Combining heterogeneous data (text + images) effectively, while preventing overfitting.

---

## ğŸš€ Solution Approach

We designed a pipeline with the following steps:

1. **Data collection & cleaning** â€” load CSVs, clean up missing/incorrect entries, etc.  
2. **Image download (parallel threads, retry logic)** and image renaming to match sample IDs.  
3. **Feature engineering**  
   - Text features: numeric extraction (values, units, pack counts), categorical flags (category, quality indicators), descriptive statistics, etc.  
   - TF-IDF features on catalog content + descriptions, then dimensionality reduction (e.g. truncated SVD) to 60 features.  
   - Image features (geometric: width/height/aspect, color statistics, brightness/contrast/saturation, texture/edge metrics, plus `img_found` flag for missing images).  
4. **Combine features** â†’ full feature set (â‰ˆ 99 features).  
5. **Outlier detection & handling** â€” using IQR-based clipping.  
6. **Model training** â€” ensemble of gradient-boosting models: LightGBM, XGBoost, CatBoost.  
7. **Prediction & submission** for test data.

---

## ğŸ“ˆ Results & Performance

- Cross-validation (5-fold) SMAPE: **38.05%** (std. dev: 0.29%)  
- Individual model performance (average across folds):  
  - LightGBM: ~38.23% SMAPE  
  - XGBoost: ~39.67% SMAPE  
  - CatBoost: ~40.12% SMAPE  
- Ensemble improves over individual models; final submissions produce predictions with range roughly matching training distribution.

---

## ğŸ“‚ Folder Structure

Amazon ML Hackathon/
â”œâ”€â”€ images/
â”œâ”€â”€ images2/
â”œâ”€â”€ catboost_info/
â”‚ â”œâ”€â”€ learn/
â”‚ â”œâ”€â”€ test/
â”‚ â””â”€â”€ tmp/
â”œâ”€â”€ dataset/
â”‚ â”œâ”€â”€ sample_test_out.csv
â”‚ â”œâ”€â”€ sample_test_ready.csv
â”‚ â”œâ”€â”€ sample_test.csv
â”‚ â”œâ”€â”€ test_out.csv
â”‚ â”œâ”€â”€ test_ready.csv
â”‚ â”œâ”€â”€ test.csv
â”‚ â”œâ”€â”€ train_ready.csv
â”‚ â””â”€â”€ train.csv
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ pycache/
â”‚ â”œâ”€â”€ aryan.ipynb
â”‚ â”œâ”€â”€ example.ipynb
â”‚ â”œâ”€â”€ submission_final_lgb.csv
â”‚ â”œâ”€â”€ submission_text_only.csv
â”‚ â”œâ”€â”€ test_out.csv
â”‚ â””â”€â”€ utils.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .DS_Store
â”œâ”€â”€ app.py
â”œâ”€â”€ app1.py
â”œâ”€â”€ app2.py
â”œâ”€â”€ app3.py
â”œâ”€â”€ app4.py
â”œâ”€â”€ code.zip
â”œâ”€â”€ dir-structure.txt
â”œâ”€â”€ Documentation_template.md
â”œâ”€â”€ download.py
â”œâ”€â”€ downTrain.py
â”œâ”€â”€ extract.py
â”œâ”€â”€ image.py
â”œâ”€â”€ lasthope.py
â”œâ”€â”€ night.py
â”œâ”€â”€ README.md
â”œâ”€â”€ rename.py
â”œâ”€â”€ sample_code.py
â”œâ”€â”€ test_image_mapping.csv
â”œâ”€â”€ test_out.csv
â”œâ”€â”€ text.py
â”œâ”€â”€ train_image_mapping.csv
â””â”€â”€ (other root-level files / scripts)



---

## ğŸ§° Getting Started / Setup

### Prerequisites

- Python 3.8+  
- RAM: 16 GB+ (recommended)  
- Disk space: ~ 20 GB (for images + intermediate files)  
- (Optional but recommended) Use a virtual environment  

### Installation

```bash
# clone the repo
git clone <your_repo_url>
cd "Amazon ML Hackathon"

# (optional) setup virtual environment
python -m venv venv
# Windows
venv\\Scripts\\activate
# then install dependencies
pip install -r requirements.txt

â–¶ï¸ How to Run

Download & prepare data: download.py, extract.py etc.

Run image download/renaming (if using images).

Run feature engineering + preprocessing.

Train models (5-fold CV + ensemble) using your training script (e.g. train_final.py).

Generate test predictions and output submission file.

Include any specific commands or parameters in your doc/comments.

ğŸ“¦ Requirements

Typical dependencies:

pandas, numpy â€” data processing

scikit-learn â€” preprocessing, TF-IDF, feature scaling

PIL / Pillow â€” image processing

LightGBM, XGBoost, CatBoost â€” modeling

(any other library you used)

You may capture versions in a requirements.txt (recommended) for reproducibility.

ğŸ§  Modeling Details

Models: LightGBM, XGBoost, CatBoost

Ensemble weights: LightGBM 50%, XGBoost 30%, CatBoost 20%

Training config: iterations 4000 (early stopping), learning rate 0.005, subsample 0.75, feature fraction 0.75, regularization L1 = 3.0, L2 = 3.0

Cross-validation: 5 folds

Outlier handling: IQR-based clipping

Feature scaling: RobustScaler (less sensitive to outliers)

ğŸ“Š Evaluation Metric (SMAPE)
SMAPE = (100 / n) * Î£ |predicted âˆ’ actual| / ((|predicted| + |actual|) / 2)


Symmetric â€” treats over- and under-predictions equally

Suitable for data with wide price ranges (handles scale differences)

Interpretation:

SMAPE < 30%: Excellent

SMAPE 30â€“40%: Good

SMAPE 40â€“55%: Acceptable

SMAPE > 55%: Poor

Our CV result: 52.05% â€” meets "Acceptable" criteria.

âš ï¸ Challenges & Lessons Learned

Downloading ~150,000 images sequentially would be too slow â€” solved by multithreaded downloader with retries.

Some products lacked images â†’ used median imputation and image-found flag.

Text descriptions were highly variable (length, formatting) â€” required robust parsing for value/unit/pack extraction.

Outliers in price distribution required careful handling â€” opted for clipping instead of removal.

Ensemble modeling with strong regularization (L1 + L2) improved generalization over individual models.

ğŸ”® Future Work & Extensions

Extract brand names using Named-Entity Recognition from text.

Parse bullet-point descriptions into structured metadata (e.g. pack size, flavor, variants).

Explore neural-network based image-text fusion (instead of handcrafted image features).

Implement a stacked ensemble (meta-learner) for further improvement.

Build a production-ready API for real-time pricing predictions, with model monitoring and update pipeline.

ğŸ“š Acknowledgements & References

Libraries & Tools: LightGBM, XGBoost, CatBoost, scikit-learn, pandas, numpy, Pillow

Methodologies: TF-IDF, ensemble learning, IQR-based outlier treatment, robust scaling

Inspired by community best practices for ML project structure and README documentation.


---

If you like, I can **generate a fully-populated** `requirements.txt` (with versions) and a `.gitignore` together, so you have ready-to-push repo.
::contentReference[oaicite:3]{index=3}
