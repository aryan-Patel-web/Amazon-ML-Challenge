"""
AMAZON ML CHALLENGE - FINAL MULTI-MODAL SOLUTION
Text + Image Features for TOP 10 Performance
Expected: < 44% SMAPE for TOP 10!
"""

import os
import pandas as pd
import numpy as np
import re
from tqdm import tqdm
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import joblib

# Image Processing
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights
import requests
from io import BytesIO

print("="*80)
print("AMAZON ML CHALLENGE - MULTI-MODAL SOLUTION (TEXT + IMAGES)")
print("Expected: < 44% SMAPE for TOP 10!")
print("="*80)

# ============================================================================
# BACKUP EXISTING FILES
# ============================================================================
def backup_file(filename):
    """Backup file if it exists"""
    if os.path.exists(filename):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = filename.replace('.csv', f'_backup_{timestamp}.csv').replace('.pkl', f'_backup_{timestamp}.pkl')
        os.rename(filename, backup_name)
        print(f"  âœ“ Backed up: {filename} â†’ {backup_name}")
        return backup_name
    return None

# ============================================================================
# STEP 1: LOAD DATA
# ============================================================================
print("\n[1/7] Loading data...")
DATASET_FOLDER = 'dataset/'

train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))
test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))

print(f"Train: {len(train):,} samples")
print(f"Test: {len(test):,} samples")
print(f"\nPrice Distribution (Train):")
print(train['price'].describe())

# ============================================================================
# STEP 2: IMAGE FEATURE EXTRACTION
# ============================================================================
print("\n[2/7] Extracting image features (this takes 15-20 min)...")

def load_image_from_url_or_local(image_link, sample_id, dataset_type='train'):
    """Load image from local folder or URL"""
    # Try local first
    local_path = f'images/{dataset_type}/{sample_id}.jpg'
    if os.path.exists(local_path):
        try:
            img = Image.open(local_path).convert('RGB')
            return img
        except:
            pass
    
    # Fallback to URL
    try:
        response = requests.get(image_link, timeout=5)
        img = Image.open(BytesIO(response.content)).convert('RGB')
        return img
    except:
        # Return blank image if failed
        return Image.new('RGB', (224, 224), color='gray')

def extract_image_features_batch(df, dataset_type='train'):
    """Extract image features using EfficientNet"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Load pre-trained model
    weights = EfficientNet_V2_S_Weights.DEFAULT
    model = efficientnet_v2_s(weights=weights)
    # Remove classifier, keep feature extractor
    model = torch.nn.Sequential(*list(model.children())[:-1])
    model = model.to(device)
    model.eval()
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    all_features = []
    batch_size = 32
    
    for i in tqdm(range(0, len(df), batch_size), desc=f"Processing {dataset_type} images"):
        batch_df = df.iloc[i:i+batch_size]
        batch_images = []
        
        for idx, row in batch_df.iterrows():
            img = load_image_from_url_or_local(row['image_link'], row['sample_id'], dataset_type)
            img_tensor = transform(img)
            batch_images.append(img_tensor)
        
        if batch_images:
            batch_tensor = torch.stack(batch_images).to(device)
            
            with torch.no_grad():
                features = model(batch_tensor)
                features = features.squeeze()
                if len(features.shape) == 1:
                    features = features.unsqueeze(0)
                features = features.cpu().numpy()
                all_features.append(features)
    
    return np.vstack(all_features)

# Extract image features
print("Extracting TRAIN image features...")
train_img_features = extract_image_features_batch(train, 'train')
train_img_df = pd.DataFrame(train_img_features, columns=[f'img_{i}' for i in range(train_img_features.shape[1])])

print("Extracting TEST image features...")
test_img_features = extract_image_features_batch(test, 'test')
test_img_df = pd.DataFrame(test_img_features, columns=[f'img_{i}' for i in range(test_img_features.shape[1])])

print(f"âœ“ Image features: {train_img_df.shape[1]} dimensions")

# ============================================================================
# STEP 3: TEXT FEATURE ENGINEERING
# ============================================================================
print("\n[3/7] Extracting text features...")

def extract_text_features(df):
    """Extract text features"""
    features_list = []
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Text features"):
        text = str(row['catalog_content']).lower()
        original = str(row['catalog_content'])
        
        feat = {}
        
        # VALUE and UNIT
        value_match = re.search(r'value:\s*(\d+\.?\d*)', text)
        feat['value'] = float(value_match.group(1)) if value_match else 0.0
        feat['value_log'] = np.log1p(feat['value'])
        feat['value_sqrt'] = np.sqrt(feat['value'])
        
        unit_match = re.search(r'unit:\s*([^\n]+)', text)
        unit_str = unit_match.group(1).strip() if unit_match else 'unknown'
        
        if any(u in unit_str for u in ['ounce', 'oz', 'pound', 'lb', 'gram', 'g', 'kg']):
            feat['unit_type'] = 'weight'
        elif any(u in unit_str for u in ['fl oz', 'fluid', 'liter', 'l', 'ml', 'gallon']):
            feat['unit_type'] = 'volume'
        elif any(u in unit_str for u in ['count', 'ct', 'piece', 'pc']):
            feat['unit_type'] = 'count'
        else:
            feat['unit_type'] = 'other'
        
        # Text stats
        words = text.split()
        feat['text_len'] = len(text)
        feat['word_count'] = len(words)
        feat['unique_words'] = len(set(words))
        feat['bullet_count'] = original.count('Bullet Point')
        feat['has_description'] = 1 if 'product description' in text else 0
        
        # Pack count
        ipq = 1
        for pattern in [r'ipq[:\s]*(\d+)', r'pack[:\s]*of[:\s]*(\d+)', r'\(pack\s+of\s+(\d+)\)']:
            match = re.search(pattern, text)
            if match:
                try:
                    val = int(match.group(1))
                    if 1 <= val <= 100:
                        ipq = val
                        break
                except:
                    pass
        
        feat['pack_count'] = ipq
        feat['pack_count_log'] = np.log1p(ipq)
        feat['unit_quantity'] = feat['value'] / max(ipq, 1)
        feat['total_volume'] = feat['value'] * ipq
        
        # Categories
        categories = {
            'food': ['food', 'snack', 'candy', 'sauce', 'seasoning', 'tea', 'coffee'],
            'electronics': ['phone', 'laptop', 'tablet', 'computer', 'camera'],
            'clothing': ['shirt', 'pant', 'dress', 'shoe'],
            'home': ['furniture', 'table', 'chair', 'bed'],
            'kitchen': ['cookware', 'pan', 'pot', 'blender'],
            'beauty': ['cosmetic', 'perfume', 'makeup']
        }
        
        for cat_name, keywords in categories.items():
            feat[f'cat_{cat_name}'] = 1 if any(kw in text for kw in keywords) else 0
        
        # Quality
        feat['premium_count'] = sum(1 for w in ['premium', 'luxury', 'pro', 'gourmet', 'original'] if w in text)
        feat['gluten_free'] = 1 if 'gluten free' in text or 'gluten-free' in text else 0
        feat['organic'] = 1 if 'organic' in text else 0
        
        features_list.append(feat)
    
    return pd.DataFrame(features_list)

train_text_features = extract_text_features(train)
test_text_features = extract_text_features(test)

# Encode categorical
le_unit = LabelEncoder()
train_text_features['unit_type_encoded'] = le_unit.fit_transform(train_text_features['unit_type'])
test_text_features['unit_type_encoded'] = test_text_features['unit_type'].map(
    dict(zip(le_unit.classes_, le_unit.transform(le_unit.classes_)))
).fillna(0).astype(int)

train_text_features.drop('unit_type', axis=1, inplace=True)
test_text_features.drop('unit_type', axis=1, inplace=True)

print(f"âœ“ Text features: {train_text_features.shape[1]}")

# ============================================================================
# STEP 4: TF-IDF
# ============================================================================
print("\n[4/7] TF-IDF...")

tfidf = TfidfVectorizer(max_features=200, ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)
tfidf_train = tfidf.fit_transform(train['catalog_content'])
tfidf_test = tfidf.transform(test['catalog_content'])

svd = TruncatedSVD(n_components=80, random_state=42)
tfidf_train_red = svd.fit_transform(tfidf_train)
tfidf_test_red = svd.transform(tfidf_test)

tfidf_train_df = pd.DataFrame(tfidf_train_red, columns=[f'tfidf_{i}' for i in range(80)])
tfidf_test_df = pd.DataFrame(tfidf_test_red, columns=[f'tfidf_{i}' for i in range(80)])

print(f"âœ“ TF-IDF: {tfidf_train_df.shape[1]} features")

# ============================================================================
# STEP 5: COMBINE ALL FEATURES
# ============================================================================
print("\n[5/7] Combining all features...")

X_train = pd.concat([
    train_text_features.reset_index(drop=True),
    tfidf_train_df,
    train_img_df
], axis=1)

X_test = pd.concat([
    test_text_features.reset_index(drop=True),
    tfidf_test_df,
    test_img_df
], axis=1)

y_train = train['price'].values

print(f"âœ“ Total features: {X_train.shape[1]}")
print(f"  - Text: {train_text_features.shape[1]}")
print(f"  - TF-IDF: {tfidf_train_df.shape[1]}")
print(f"  - Image: {train_img_df.shape[1]}")

# ============================================================================
# STEP 6: TRAIN ADVANCED ENSEMBLE
# ============================================================================
print("\n[6/7] Training multi-modal ensemble (7-fold CV)...")

def smape(y_true, y_pred):
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_true - y_pred) / denominator
    diff[denominator == 0] = 0.0
    return 100 * np.mean(diff)

class MultiModalEnsemble:
    def __init__(self):
        self.lgb_models = []
        self.xgb_models = []
        self.cat_models = []
        self.scaler = StandardScaler()
        
    def train(self, X, y, n_folds=7):
        X_scaled = self.scaler.fit_transform(X)
        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
        oof = np.zeros(len(X))
        
        for fold, (tr_idx, val_idx) in enumerate(kf.split(X_scaled)):
            print(f"\nFold {fold + 1}/{n_folds}")
            
            X_tr, X_val = X_scaled[tr_idx], X_scaled[val_idx]
            y_tr, y_val = y[tr_idx], y[val_idx]
            
            # LightGBM
            lgb_model = lgb.LGBMRegressor(
                n_estimators=2500, learning_rate=0.015, num_leaves=60, max_depth=14,
                min_child_samples=30, subsample=0.8, colsample_bytree=0.8,
                reg_alpha=0.1, reg_lambda=0.1, random_state=42, n_jobs=-1, verbose=-1
            )
            lgb_model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],
                         callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)])
            
            # XGBoost
            xgb_model = xgb.XGBRegressor(
                n_estimators=2500, learning_rate=0.015, max_depth=14, min_child_weight=5,
                subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,
                random_state=42, tree_method='hist', n_jobs=-1, verbosity=0
            )
            xgb_model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)
            
            # CatBoost
            cat_model = CatBoostRegressor(
                iterations=2000, learning_rate=0.015, depth=12, l2_leaf_reg=5,
                random_seed=42, verbose=0
            )
            cat_model.fit(X_tr, y_tr, eval_set=(X_val, y_val),
                         early_stopping_rounds=200, verbose=False)
            
            lgb_pred = lgb_model.predict(X_val)
            xgb_pred = xgb_model.predict(X_val)
            cat_pred = cat_model.predict(X_val)
            
            fold_pred = 0.4 * lgb_pred + 0.35 * xgb_pred + 0.25 * cat_pred
            oof[val_idx] = fold_pred
            
            print(f"  LGB: {smape(y_val, lgb_pred):.3f} | XGB: {smape(y_val, xgb_pred):.3f} | CAT: {smape(y_val, cat_pred):.3f} | Ens: {smape(y_val, fold_pred):.3f}")
            
            self.lgb_models.append(lgb_model)
            self.xgb_models.append(xgb_model)
            self.cat_models.append(cat_model)
        
        final_smape = smape(y, oof)
        print(f"\n{'='*60}")
        print(f"â­ Overall CV SMAPE: {final_smape:.4f}% â­")
        print(f"{'='*60}")
        return oof
    
    def predict(self, X):
        X_scaled = self.scaler.transform(X)
        preds = []
        
        for lgb_m, xgb_m, cat_m in zip(self.lgb_models, self.xgb_models, self.cat_models):
            lgb_p = lgb_m.predict(X_scaled)
            xgb_p = xgb_m.predict(X_scaled)
            cat_p = cat_m.predict(X_scaled)
            fold_p = 0.4 * lgb_p + 0.35 * xgb_p + 0.25 * cat_p
            preds.append(fold_p)
        
        return np.maximum(np.mean(preds, axis=0), 0.01)

model = MultiModalEnsemble()
oof = model.train(X_train.values, y_train, n_folds=7)

# ============================================================================
# STEP 7: PREDICT & SAVE
# ============================================================================
print("\n[7/7] Predicting...")

predictions = model.predict(X_test.values)

submission = pd.DataFrame({
    'sample_id': test['sample_id'],
    'price': predictions
})

# Backup
print("\n[Backup] Checking for existing files...")
backup_file('test_out.csv')
backup_file('multimodal_model.pkl')

submission.to_csv('test_out.csv', index=False)
joblib.dump(model, 'multimodal_model.pkl')

print(f"\n{'='*80}")
print("âœ… MULTI-MODAL TRAINING COMPLETE!")
print(f"{'='*80}")

print(f"\nPredictions:")
print(f"  Mean: ${predictions.mean():.2f}")
print(f"  Median: ${np.median(predictions):.2f}")
print(f"  Min: ${predictions.min():.2f}")
print(f"  Max: ${predictions.max():.2f}")

print(f"\nðŸŽ¯ CV SMAPE: {smape(y_train, oof):.2f}%")
print(f"ðŸŽ¯ Previous: 58.93%")
print(f"ðŸŽ¯ Target: < 44% for TOP 10")
print(f"ðŸŽ¯ Improvement: {58.93 - smape(y_train, oof):.1f}%!")

print(f"\n{'='*80}")
print("ðŸš€ READY TO SUBMIT! EXPECTED RANK: TOP 10-20!")
print(f"{'='*80}")