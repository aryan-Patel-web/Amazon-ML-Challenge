{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5fa091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "# %pip install catboost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import joblib\n",
    "\n",
    "# Deep Learning for Text\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 10 STRATEGY - ADVANCED MULTI-MODAL ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "DATASET_FOLDER = '../dataset/'\n",
    "train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "\n",
    "print(f\"Train: {len(train):,} | Test: {len(test):,}\")\n",
    "print(f\"\\nPrice Distribution:\")\n",
    "print(train['price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79623c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AMAZON ML CHALLENGE - TOP 5 SOLUTION\n",
    "Complete pipeline in one file\n",
    "Expected SMAPE: 45-50% ‚Üí Target: < 47% for TOP 5\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import joblib\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AMAZON ML CHALLENGE - TOP 5 COMPLETE SOLUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1/6] Loading data...\")\n",
    "DATASET_FOLDER = 'dataset/'\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "\n",
    "print(f\"Train: {len(train):,} samples\")\n",
    "print(f\"Test: {len(test):,} samples\")\n",
    "print(f\"\\nPrice Distribution (Train):\")\n",
    "print(train['price'].describe())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: ADVANCED FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n[2/6] Extracting features (this takes 2-3 minutes)...\")\n",
    "\n",
    "def extract_all_features(df):\n",
    "    \"\"\"Extract comprehensive features\"\"\"\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        text = str(row['catalog_content']).lower()\n",
    "        original = str(row['catalog_content'])\n",
    "        \n",
    "        feat = {}\n",
    "        \n",
    "        # === TEXT STATS ===\n",
    "        words = text.split()\n",
    "        feat['text_len'] = len(text)\n",
    "        feat['word_count'] = len(words)\n",
    "        feat['unique_words'] = len(set(words))\n",
    "        feat['avg_word_len'] = feat['text_len'] / max(feat['word_count'], 1)\n",
    "        feat['capital_ratio'] = sum(1 for c in original if c.isupper()) / max(len(original), 1)\n",
    "        feat['digit_ratio'] = sum(1 for c in text if c.isdigit()) / max(len(text), 1)\n",
    "        \n",
    "        # === IPQ (CRITICAL!) ===\n",
    "        ipq = 1\n",
    "        for pattern in [r'ipq[:\\s]*(\\d+)', r'pack[:\\s]*of[:\\s]*(\\d+)', r'(\\d+)[:\\s]*pack',\n",
    "                       r'quantity[:\\s]*(\\d+)', r'set[:\\s]*of[:\\s]*(\\d+)', r'(\\d+)[:\\s]*piece']:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                try:\n",
    "                    val = int(match.group(1))\n",
    "                    if 1 <= val <= 100:\n",
    "                        ipq = val\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        feat['ipq'] = ipq\n",
    "        feat['ipq_log'] = np.log1p(ipq)\n",
    "        feat['ipq_sqrt'] = np.sqrt(ipq)\n",
    "        feat['ipq_sq'] = ipq ** 2\n",
    "        feat['is_multipack'] = 1 if ipq > 1 else 0\n",
    "        \n",
    "        # === NUMBERS ===\n",
    "        numbers = [float(n) for n in re.findall(r'\\d+\\.?\\d*', text) if 0 < float(n) < 1000000]\n",
    "        if numbers:\n",
    "            feat['num_count'] = len(numbers)\n",
    "            feat['num_max'] = max(numbers)\n",
    "            feat['num_min'] = min(numbers)\n",
    "            feat['num_mean'] = np.mean(numbers)\n",
    "            feat['num_median'] = np.median(numbers)\n",
    "            feat['num_std'] = np.std(numbers) if len(numbers) > 1 else 0\n",
    "            feat['num_sum'] = sum(numbers)\n",
    "            feat['num_range'] = max(numbers) - min(numbers)\n",
    "        else:\n",
    "            for k in ['num_count', 'num_max', 'num_min', 'num_mean', 'num_median', \n",
    "                     'num_std', 'num_sum', 'num_range']:\n",
    "                feat[k] = 0\n",
    "        \n",
    "        # === STORAGE (GB/TB) ===\n",
    "        storage_gb = 0\n",
    "        for match in re.finditer(r'(\\d+)\\s*(gb|tb|mb)', text):\n",
    "            val = int(match.group(1))\n",
    "            unit = match.group(2)\n",
    "            if unit == 'tb':\n",
    "                val *= 1000\n",
    "            elif unit == 'mb':\n",
    "                val *= 0.001\n",
    "            storage_gb += val\n",
    "        \n",
    "        feat['storage_gb'] = storage_gb\n",
    "        feat['storage_log'] = np.log1p(storage_gb)\n",
    "        feat['has_storage'] = 1 if storage_gb > 0 else 0\n",
    "        \n",
    "        # RAM\n",
    "        ram_match = re.search(r'(\\d+)\\s*gb\\s*ram', text)\n",
    "        feat['ram_gb'] = int(ram_match.group(1)) if ram_match else 0\n",
    "        \n",
    "        # === BRANDS ===\n",
    "        brands = {\n",
    "            'apple': 5, 'samsung': 4, 'sony': 4, 'lg': 3, 'dell': 3, 'hp': 3,\n",
    "            'lenovo': 3, 'asus': 3, 'microsoft': 5, 'google': 4, 'nike': 4,\n",
    "            'adidas': 4, 'puma': 3, 'canon': 4, 'nikon': 4, 'bose': 5,\n",
    "            'bosch': 4, 'philips': 3, 'xiaomi': 2, 'oneplus': 3\n",
    "        }\n",
    "        \n",
    "        feat['brand_score'] = sum(score for brand, score in brands.items() if brand in text)\n",
    "        feat['brand_count'] = sum(1 for brand in brands if brand in text)\n",
    "        feat['has_premium_brand'] = 1 if feat['brand_score'] >= 4 else 0\n",
    "        \n",
    "        # Top brands individually\n",
    "        for brand in ['apple', 'samsung', 'sony', 'nike', 'microsoft', 'canon', 'bose']:\n",
    "            feat[f'brand_{brand}'] = 1 if brand in text else 0\n",
    "        \n",
    "        # === CATEGORIES ===\n",
    "        categories = {\n",
    "            'electronics': ['phone', 'laptop', 'tablet', 'computer', 'tv', 'camera', \n",
    "                          'headphone', 'speaker', 'watch', 'earphone', 'monitor'],\n",
    "            'clothing': ['shirt', 'pant', 'dress', 'jean', 'jacket', 'shoe', 'sneaker'],\n",
    "            'home': ['furniture', 'table', 'chair', 'sofa', 'bed', 'lamp'],\n",
    "            'kitchen': ['cookware', 'pan', 'pot', 'knife', 'blender', 'oven'],\n",
    "            'beauty': ['cosmetic', 'perfume', 'makeup', 'skincare', 'shampoo'],\n",
    "            'sports': ['fitness', 'gym', 'yoga', 'dumbbell', 'cycle', 'ball'],\n",
    "            'books': ['book', 'novel', 'textbook', 'guide'],\n",
    "            'toys': ['toy', 'game', 'puzzle', 'doll']\n",
    "        }\n",
    "        \n",
    "        cat_scores = {}\n",
    "        for cat_name, keywords in categories.items():\n",
    "            count = sum(1 for kw in keywords if kw in text)\n",
    "            feat[f'cat_{cat_name}'] = 1 if count > 0 else 0\n",
    "            cat_scores[cat_name] = count\n",
    "        \n",
    "        # Primary category\n",
    "        primary_cat = max(cat_scores, key=cat_scores.get) if max(cat_scores.values()) > 0 else 'other'\n",
    "        feat['primary_category'] = primary_cat\n",
    "        \n",
    "        # === QUALITY ===\n",
    "        premium = ['premium', 'luxury', 'pro', 'plus', 'ultra', 'max', 'deluxe', \n",
    "                  'elite', 'supreme', 'professional', 'advanced']\n",
    "        budget = ['basic', 'standard', 'lite', 'mini', 'essential', 'simple']\n",
    "        \n",
    "        feat['premium_count'] = sum(1 for w in premium if w in text)\n",
    "        feat['budget_count'] = sum(1 for w in budget if w in text)\n",
    "        feat['quality_score'] = feat['premium_count'] - feat['budget_count']\n",
    "        feat['is_premium'] = 1 if feat['premium_count'] > feat['budget_count'] else 0\n",
    "        \n",
    "        # === SPECIAL FEATURES ===\n",
    "        features_kw = {\n",
    "            'wireless': ['wireless', 'bluetooth', 'wifi'],\n",
    "            'original': ['original', 'genuine'],\n",
    "            'warranty': ['warranty', 'guarantee'],\n",
    "            'waterproof': ['waterproof', 'water-resistant'],\n",
    "            'rechargeable': ['rechargeable', 'battery'],\n",
    "            'smart': ['smart', 'ai'],\n",
    "            'hd': ['hd', '4k', 'uhd']\n",
    "        }\n",
    "        \n",
    "        for feat_name, keywords in features_kw.items():\n",
    "            feat[f'feat_{feat_name}'] = 1 if any(kw in text for kw in keywords) else 0\n",
    "        \n",
    "        # === MATERIALS ===\n",
    "        expensive_mat = ['leather', 'gold', 'silver', 'steel', 'aluminum', 'titanium', 'wood']\n",
    "        cheap_mat = ['plastic', 'rubber', 'synthetic']\n",
    "        \n",
    "        feat['expensive_mat'] = 1 if any(m in text for m in expensive_mat) else 0\n",
    "        feat['cheap_mat'] = 1 if any(m in text for m in cheap_mat) else 0\n",
    "        \n",
    "        # === MISC ===\n",
    "        feat['is_new'] = 1 if any(w in text for w in ['new', 'latest', '2024', '2025']) else 0\n",
    "        feat['color_count'] = sum(1 for c in ['black', 'white', 'red', 'blue', 'green'] if c in text)\n",
    "        feat['has_size'] = 1 if any(s in text for s in ['small', 'medium', 'large', 'xl']) else 0\n",
    "        \n",
    "        features_list.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_all_features(train)\n",
    "test_features = extract_all_features(test)\n",
    "\n",
    "# Encode categorical\n",
    "le = LabelEncoder()\n",
    "train_features['primary_category_encoded'] = le.fit_transform(train_features['primary_category'])\n",
    "test_features['primary_category_encoded'] = le.transform(test_features['primary_category'])\n",
    "\n",
    "train_features.drop('primary_category', axis=1, inplace=True)\n",
    "test_features.drop('primary_category', axis=1, inplace=True)\n",
    "\n",
    "print(f\"‚úì Extracted {train_features.shape[1]} handcrafted features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: TF-IDF TEXT FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[3/6] TF-IDF text vectorization...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=400,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "tfidf_train = tfidf.fit_transform(train['catalog_content'])\n",
    "tfidf_test = tfidf.transform(test['catalog_content'])\n",
    "\n",
    "# Reduce dimensions\n",
    "svd = TruncatedSVD(n_components=120, random_state=42)\n",
    "tfidf_train_red = svd.fit_transform(tfidf_train)\n",
    "tfidf_test_red = svd.transform(tfidf_test)\n",
    "\n",
    "tfidf_train_df = pd.DataFrame(tfidf_train_red, columns=[f'tfidf_{i}' for i in range(120)])\n",
    "tfidf_test_df = pd.DataFrame(tfidf_test_red, columns=[f'tfidf_{i}' for i in range(120)])\n",
    "\n",
    "print(f\"‚úì TF-IDF: {tfidf_train_df.shape[1]} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: COMBINE ALL FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n[4/6] Combining features...\")\n",
    "\n",
    "X_train = pd.concat([train_features.reset_index(drop=True), tfidf_train_df], axis=1)\n",
    "X_test = pd.concat([test_features.reset_index(drop=True), tfidf_test_df], axis=1)\n",
    "y_train = train['price'].values\n",
    "\n",
    "print(f\"‚úì Total features: {X_train.shape[1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TRAIN ADVANCED ENSEMBLE\n",
    "# ============================================================================\n",
    "print(\"\\n[5/6] Training advanced ensemble (7-fold CV)...\")\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "class TopEnsemble:\n",
    "    def __init__(self):\n",
    "        self.lgb_models = []\n",
    "        self.xgb_models = []\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def train(self, X, y, n_folds=7):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "        oof_predictions = np.zeros(len(X))\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "            print(f\"\\nFold {fold + 1}/{n_folds}\")\n",
    "            \n",
    "            X_tr, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # LightGBM\n",
    "            lgb_model = lgb.LGBMRegressor(\n",
    "                n_estimators=1500,\n",
    "                learning_rate=0.025,\n",
    "                num_leaves=45,\n",
    "                max_depth=12,\n",
    "                min_child_samples=25,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.05,\n",
    "                reg_lambda=0.05,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            )\n",
    "            lgb_model.fit(X_tr, y_tr, \n",
    "                         eval_set=[(X_val, y_val)],\n",
    "                         callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])\n",
    "            \n",
    "            # XGBoost\n",
    "            xgb_model = xgb.XGBRegressor(\n",
    "                n_estimators=1500,\n",
    "                learning_rate=0.025,\n",
    "                max_depth=12,\n",
    "                min_child_weight=5,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.05,\n",
    "                reg_lambda=0.05,\n",
    "                random_state=42,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            )\n",
    "            xgb_model.fit(X_tr, y_tr,\n",
    "                         eval_set=[(X_val, y_val)],\n",
    "                         verbose=False)\n",
    "            \n",
    "            # Predictions\n",
    "            lgb_pred = lgb_model.predict(X_val)\n",
    "            xgb_pred = xgb_model.predict(X_val)\n",
    "            \n",
    "            # Ensemble\n",
    "            fold_pred = 0.55 * lgb_pred + 0.45 * xgb_pred\n",
    "            oof_predictions[val_idx] = fold_pred\n",
    "            \n",
    "            fold_smape = smape(y_val, fold_pred)\n",
    "            print(f\"  LGB: {smape(y_val, lgb_pred):.4f} | XGB: {smape(y_val, xgb_pred):.4f} | Ensemble: {fold_smape:.4f}\")\n",
    "            \n",
    "            self.lgb_models.append(lgb_model)\n",
    "            self.xgb_models.append(xgb_model)\n",
    "        \n",
    "        overall_smape = smape(y, oof_predictions)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚≠ê Overall CV SMAPE: {overall_smape:.4f} ‚≠ê\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return oof_predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        predictions = []\n",
    "        \n",
    "        for lgb_m, xgb_m in zip(self.lgb_models, self.xgb_models):\n",
    "            lgb_pred = lgb_m.predict(X_scaled)\n",
    "            xgb_pred = xgb_m.predict(X_scaled)\n",
    "            fold_pred = 0.55 * lgb_pred + 0.45 * xgb_pred\n",
    "            predictions.append(fold_pred)\n",
    "        \n",
    "        final_pred = np.mean(predictions, axis=0)\n",
    "        return np.maximum(final_pred, 0.01)\n",
    "\n",
    "# Train model\n",
    "model = TopEnsemble()\n",
    "oof_preds = model.train(X_train.values, y_train, n_folds=7)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: PREDICT & SAVE\n",
    "# ============================================================================\n",
    "print(\"\\n[6/6] Predicting on test set...\")\n",
    "\n",
    "predictions = model.predict(X_test.values)\n",
    "\n",
    "# Post-processing: clip extreme predictions\n",
    "predictions = np.clip(predictions, \n",
    "                     train['price'].quantile(0.001), \n",
    "                     train['price'].quantile(0.999))\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test['sample_id'],\n",
    "    'price': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('test_out.csv', index=False)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, 'top5_model.pkl')\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"  Mean: ${predictions.mean():.2f}\")\n",
    "print(f\"  Median: ${np.median(predictions):.2f}\")\n",
    "print(f\"  Min: ${predictions.min():.2f}\")\n",
    "print(f\"  Max: ${predictions.max():.2f}\")\n",
    "print(f\"  Std: ${predictions.std():.2f}\")\n",
    "\n",
    "print(f\"\\nTrain Price Statistics:\")\n",
    "print(f\"  Mean: ${y_train.mean():.2f}\")\n",
    "print(f\"  Median: ${np.median(y_train):.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FILES CREATED:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úì test_out.csv - Your submission file\")\n",
    "print(f\"‚úì top5_model.pkl - Trained model\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUBMISSION VERIFICATION:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úì Rows: {len(submission)} (Expected: {len(test)})\")\n",
    "print(f\"‚úì Columns: {submission.columns.tolist()}\")\n",
    "print(f\"‚úì No NaN: {submission['price'].isna().sum() == 0}\")\n",
    "print(f\"‚úì All positive: {(submission['price'] > 0).all()}\")\n",
    "\n",
    "if len(submission) == len(test) and (submission['price'] > 0).all():\n",
    "    print(f\"\\nüéØ READY TO SUBMIT!\")\n",
    "    print(f\"\\nüìä EXPECTED PERFORMANCE:\")\n",
    "    print(f\"  CV SMAPE: {smape(y_train, oof_preds):.2f}%\")\n",
    "    print(f\"  Target: < 47% for TOP 5\")\n",
    "    print(f\"  Your improvement: {69.2 - smape(y_train, oof_preds):.1f}% better!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Check submission file!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üöÄ UPLOAD test_out.csv TO COMPETITION PORTAL!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13e651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/6] TF-IDF vectorization...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[2/6] TF-IDF vectorization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m tfidf = TfidfVectorizer(\n\u001b[32m      4\u001b[39m     max_features=\u001b[32m500\u001b[39m,\n\u001b[32m      5\u001b[39m     ngram_range=(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     strip_accents=\u001b[33m'\u001b[39m\u001b[33municode\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m tfidf_train = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcatalog_content\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m tfidf_test = tfidf.transform(test[\u001b[33m'\u001b[39m\u001b[33mcatalog_content\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Reduce dimensions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\python-download\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\python-download\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\python-download\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\python-download\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\python-download\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    109\u001b[39m             doc = ngrams(doc, stop_words)\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m             doc = \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\python-download\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:264\u001b[39m, in \u001b[36m_VectorizerMixin._word_ngrams\u001b[39m\u001b[34m(self, tokens, stop_words)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_n, \u001b[38;5;28mmin\u001b[39m(max_n + \u001b[32m1\u001b[39m, n_original_tokens + \u001b[32m1\u001b[39m)):\n\u001b[32m    263\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_original_tokens - n + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m             \u001b[43mtokens_append\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspace_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# TF-IDF for text understanding\n",
    "print(\"\\n[2/6] TF-IDF vectorization...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=500,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "tfidf_train = tfidf.fit_transform(train['catalog_content'])\n",
    "tfidf_test = tfidf.transform(test['catalog_content'])\n",
    "\n",
    "# Reduce dimensions\n",
    "svd = TruncatedSVD(n_components=150, random_state=42)\n",
    "tfidf_train_svd = svd.fit_transform(tfidf_train)\n",
    "tfidf_test_svd = svd.transform(tfidf_test)\n",
    "\n",
    "tfidf_train_df = pd.DataFrame(tfidf_train_svd, columns=[f'tfidf_{i}' for i in range(150)])\n",
    "tfidf_test_df = pd.DataFrame(tfidf_test_svd, columns=[f'tfidf_{i}' for i in range(150)])\n",
    "\n",
    "print(f\"‚úì TF-IDF: {tfidf_train_df.shape[1]} features\")\n",
    "\n",
    "# Optional: Add BERT if you have time (improves score by 5-10%)\n",
    "# Uncomment if you want to use BERT:\n",
    "\"\"\"\n",
    "print(\"\\n[3/6] BERT embeddings (this takes time)...\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=16):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = AutoModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = TextDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"BERT\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "train_bert = get_bert_embeddings(train['catalog_content'].values)\n",
    "test_bert = get_bert_embeddings(test['catalog_content'].values)\n",
    "\n",
    "train_bert_df = pd.DataFrame(train_bert, columns=[f'bert_{i}' for i in range(train_bert.shape[1])])\n",
    "test_bert_df = pd.DataFrame(test_bert, columns=[f'bert_{i}' for i in range(test_bert.shape[1])])\n",
    "\n",
    "# Combine all features\n",
    "X_train_full = pd.concat([train_features.reset_index(drop=True), \n",
    "                          tfidf_train_df, \n",
    "                          train_bert_df], axis=1)\n",
    "X_test_full = pd.concat([test_features.reset_index(drop=True), \n",
    "                         tfidf_test_df, \n",
    "                         test_bert_df], axis=1)\n",
    "\"\"\"\n",
    "\n",
    "# Without BERT (faster):\n",
    "X_train_full = pd.concat([train_features.reset_index(drop=True), tfidf_train_df], axis=1)\n",
    "X_test_full = pd.concat([test_features.reset_index(drop=True), tfidf_test_df], axis=1)\n",
    "\n",
    "y_train = train['price'].values\n",
    "\n",
    "print(f\"‚úì Total features: {X_train_full.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cab286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/6] Training advanced ensemble...\")\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "class AdvancedEnsemble:\n",
    "    def __init__(self):\n",
    "        self.lgb_models = []\n",
    "        self.xgb_models = []\n",
    "        self.cat_models = []\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def train(self, X, y, n_folds=7):  # 7 folds for better stability\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "        oof_predictions = np.zeros(len(X))\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            X_train_fold, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "            y_train_fold, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Model 1: LightGBM\n",
    "            print(\"Training LightGBM...\")\n",
    "            lgb_model = lgb.LGBMRegressor(\n",
    "                n_estimators=2000,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=50,\n",
    "                max_depth=12,\n",
    "                min_child_samples=25,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.05,\n",
    "                reg_lambda=0.05,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            )\n",
    "            lgb_model.fit(X_train_fold, y_train_fold,\n",
    "                         eval_set=[(X_val, y_val)],\n",
    "                         callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])\n",
    "            \n",
    "            # Model 2: XGBoost\n",
    "            print(\"Training XGBoost...\")\n",
    "            xgb_model = xgb.XGBRegressor(\n",
    "                n_estimators=2000,\n",
    "                learning_rate=0.02,\n",
    "                max_depth=12,\n",
    "                min_child_weight=5,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.05,\n",
    "                reg_lambda=0.05,\n",
    "                random_state=42,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            )\n",
    "            xgb_model.fit(X_train_fold, y_train_fold,\n",
    "                         eval_set=[(X_val, y_val)],\n",
    "                         verbose=False)\n",
    "            \n",
    "            # Model 3: CatBoost\n",
    "            print(\"Training CatBoost...\")\n",
    "            cat_model = CatBoostRegressor(\n",
    "                iterations=2000,\n",
    "                learning_rate=0.02,\n",
    "                depth=10,\n",
    "                l2_leaf_reg=5,\n",
    "                random_seed=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            cat_model.fit(X_train_fold, y_train_fold,\n",
    "                         eval_set=(X_val, y_val),\n",
    "                         early_stopping_rounds=100,\n",
    "                         verbose=False)\n",
    "            \n",
    "            # Predictions\n",
    "            lgb_pred = lgb_model.predict(X_val)\n",
    "            xgb_pred = xgb_model.predict(X_val)\n",
    "            cat_pred = cat_model.predict(X_val)\n",
    "            \n",
    "            # Weighted ensemble (tune these weights!)\n",
    "            fold_pred = 0.4 * lgb_pred + 0.35 * xgb_pred + 0.25 * cat_pred\n",
    "            oof_predictions[val_idx] = fold_pred\n",
    "            \n",
    "            # Individual model scores\n",
    "            lgb_smape = smape(y_val, lgb_pred)\n",
    "            xgb_smape = smape(y_val, xgb_pred)\n",
    "            cat_smape = smape(y_val, cat_pred)\n",
    "            fold_smape = smape(y_val, fold_pred)\n",
    "            \n",
    "            print(f\"  LGB: {lgb_smape:.4f}\")\n",
    "            print(f\"  XGB: {xgb_smape:.4f}\")\n",
    "            print(f\"  CAT: {cat_smape:.4f}\")\n",
    "            print(f\"‚úì Ensemble: {fold_smape:.4f}\")\n",
    "            \n",
    "            self.lgb_models.append(lgb_model)\n",
    "            self.xgb_models.append(xgb_model)\n",
    "            self.cat_models.append(cat_model)\n",
    "        \n",
    "        overall_smape = smape(y, oof_predictions)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Overall CV SMAPE: {overall_smape:.4f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return oof_predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        predictions = []\n",
    "        \n",
    "        for lgb_m, xgb_m, cat_m in zip(self.lgb_models, self.xgb_models, self.cat_models):\n",
    "            lgb_pred = lgb_m.predict(X_scaled)\n",
    "            xgb_pred = xgb_m.predict(X_scaled)\n",
    "            cat_pred = cat_m.predict(X_scaled)\n",
    "            \n",
    "            fold_pred = 0.4 * lgb_pred + 0.35 * xgb_pred + 0.25 * cat_pred\n",
    "            predictions.append(fold_pred)\n",
    "        \n",
    "        final_pred = np.mean(predictions, axis=0)\n",
    "        return np.maximum(final_pred, 0.01)\n",
    "\n",
    "model = AdvancedEnsemble()\n",
    "oof_preds = model.train(X_train_full.values, y_train, n_folds=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88636bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUBMISSION VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "submission_check = pd.read_csv('test_out.csv')\n",
    "print(f\"‚úì Rows: {len(submission_check)} (Expected: {len(test)})\")\n",
    "print(f\"‚úì Columns: {submission_check.columns.tolist()}\")\n",
    "print(f\"‚úì No NaN: {submission_check['price'].isna().sum() == 0}\")\n",
    "print(f\"‚úì All positive: {(submission_check['price'] > 0).all()}\")\n",
    "\n",
    "if len(submission_check) == len(test) and (submission_check['price'] > 0).all():\n",
    "    print(f\"\\n‚úÖ SUBMISSION READY FOR UPLOAD!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Fix issues before submitting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7c9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286c824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4258bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45cf8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d6762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cabd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a4507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae4290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613e6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d12c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa37589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10694d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e648e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2b81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d8382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff8515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb432a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bd66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d0fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d15f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c88f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca40f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95afd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
